{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k=3, max_iter=100, tol=1e-4):\n",
    "    # Inisialisasi centroid secara acak\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    centroids = X[random_indices]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Hitung jarak\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        # Assign cluster\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Update centroid\n",
    "        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n",
    "        \n",
    "        # Check konvergensi\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def _softmax(self, Z):\n",
    "        # Z shape: (n_samples, n_classes)\n",
    "        # Stabilization trick: Z - np.max(Z, axis=1, keepdims=True)\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # X shape: (n_samples, n_features)\n",
    "        # y shape: (n_samples,) berisi label kelas (0,1,...,K-1)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # One-hot encoding untuk y\n",
    "        Y_one_hot = np.zeros((n_samples, n_classes))\n",
    "        for idx, val in enumerate(y):\n",
    "            Y_one_hot[idx, np.where(self.classes==val)[0][0]] = 1\n",
    "        \n",
    "        # Inisialisasi parameter\n",
    "        # Tambahkan bias ke X (optional, kita bisa menyertakan bias)\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])  # menambahkan kolom ones di depan\n",
    "        n_features += 1\n",
    "        \n",
    "        self.W = np.zeros((n_features, n_classes))  # bobot: (n_features, n_classes)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            # Forward\n",
    "            Z = X.dot(self.W)  # (n_samples, n_classes)\n",
    "            A = self._softmax(Z)  # (n_samples, n_classes)\n",
    "            \n",
    "            # Gradient\n",
    "            grad = (1/n_samples) * X.T.dot(A - Y_one_hot)  # (n_features, n_classes)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W -= self.learning_rate * grad\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # X shape: (n_samples, n_features)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        Z = X.dot(self.W)\n",
    "        return self._softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        class_indices = np.argmax(proba, axis=1)\n",
    "        return self.classes[class_indices]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
